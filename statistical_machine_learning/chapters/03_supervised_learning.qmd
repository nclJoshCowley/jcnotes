---
title: "Supervised Learning"
---

## Notation

The notation given in [@aslett2021] is consistent with the conventions the 
    author is accustomed to and will not be repeated here.

## Problem Setting

We consider three problem types concerning supervised ML:

1. **regression**, outcome is quantitative, such as weight,

1. **classification**, outcome is qualitative, such as image recognition of a 
    dog or cat,
    
1. **density estimation**, models the distribution of the outcome.

Supervised learning assumes we have some data of size $n$,

$$
    \mathcal{D}
        = \left(
            (\mathbf{x}_1, y_1), \dots, (\mathbf{x}_n, y_n) 
        \right) \subset (\mathcal{X} \times \mathcal{Y})^n
    ,
$$

to learn from.

Where we define

- $\mathcal{X}$ to be the space containing our $d$ dimensional **predictors**,
  $\mathbf{x}_i = (x_{i1}, \dots, x_{id})^T$,
  
- $\mathcal{Y}$ is the space containing our **outcomes**, that is dependent on
  the type of problem (regression implies $\mathcal{Y} \subset \mathbb{R}$).
    
Given these data, we make a further assumption by specifying a *model* that 
defines some relationship and uncertainty.
In regression,

$$
    Y 
        = f(x) + \epsilon
    ,
$$

where $\epsilon$ is a random error term, zero mean assumed, and $f(x)$ is 
the conditional expectation, $f(x) = \mathbb{E}[Y | X = \mathbf{x}]$.

In other settings, this may take on other forms such as a 
*probabilistic classifier*, where for some categorical distribution we have
    
$$
    (Y | X = \mathbf{x})
        \sim \text{Categorical} \big(
            (p_1, \dots, p_g) = f(\mathbf{x}) 
        \big)
    ,
$$

where $f: \mathcal{X} \to [0,1]^g$, and $p_i = \Pr(Y = i | X = \mathbf{x})$ is
defined to be on the simplex $\left( \sum_i p_i = 1 \right)$.

::: {#prp-ProbabilityMeasure}

Assume that (typically) each observation $(\mathbf{x}_i, y_i)$ is an i.i.d.
realisation from some *true* but *unknown* **probability measure**
denoted $\pi_{XY}$.

:::
    
Our task is thus, we have access to some predictors, $\mathbf{x}$, but not $y$.
We want to predict these $y$ values with a model but, importantly, the model
needs to generalise well to unseen data.
    
This is because we are interested in a *random input* setting. Note that a

- *fixed input* setting assumes $X$ fixed and aims to explain the randomness
  in $Y$,

- *random input* setting attempts to explain randomness of $X$ and $Y$, 
  that is, make predictions at unknown future values of $X$.


## Loss Functions

**TODO**.
